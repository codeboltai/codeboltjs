export interface Provider {
  id: number;
  logo: string;
  name: string;
  apiUrl: string;
  category: 'codebolt' | 'cloudProviders' | 'localProviders';
}

export type SupportedProvider = 
  | "codeboltai"
  | "openai"
  | "anthropic"
  | "perplexity"
  | "lmstudio"
  | "mistral"
  | "gemini"
  | "grok"
  | "ollama"
  | "bedrock"
  | "huggingface"
  | "github"
  | "groq"
  | "replicate"
  | "openrouter"
  | "cloudflare"
  | "deepseek"
  | "zai";

export interface AWSConfig {
  accessKeyId?: string;
  secretAccessKey?: string;
  region?: string;
}

/**
 * Telemetry configuration for automatic logging of LLM calls
 *
 * Telemetry is ENABLED BY DEFAULT - all LLM operations are automatically logged
 * to './llm-telemetry.ndjson'. Set enabled: false to disable.
 */
export interface TelemetryOptions {
  /** Enable telemetry (default: true) - set to false to disable */
  enabled?: boolean;
  /** File path for telemetry logs in NDJSON format (default: './llm-telemetry.ndjson') */
  filePath?: string;
  /** Also export in OTLP format for OpenTelemetry collectors */
  exportOtlp?: boolean;
  /** Log to console (useful for development, default: false) */
  consoleLog?: boolean;
  /** Verbose console output with full JSON (default: false) */
  consoleVerbose?: boolean;
  /** Record input prompts (default: true) */
  recordInputs?: boolean;
  /** Record output completions (default: true) */
  recordOutputs?: boolean;
  /** Service name for telemetry attribution (default: 'multillm') */
  serviceName?: string;
  /** Custom metadata to attach to all telemetry spans */
  metadata?: Record<string, unknown>;
}

export interface ProviderConfig {
  aws?: AWSConfig;
  /** Telemetry configuration - enables automatic logging of all LLM calls */
  telemetry?: TelemetryOptions;
}

export interface BaseProvider {
  model: string | null;
  device_map: string | null;
  apiKey?: string | null;
  apiEndpoint: string | null;
  config?: ProviderConfig;
}

// ============================================
// Multimodal Content Types
// ============================================

/**
 * Text content part for multimodal messages
 */
export interface TextContentPart {
  type: 'text';
  /** The text content */
  text: string;
}

/**
 * Image content part for vision models
 * Supports multiple input formats: URL, base64, ArrayBuffer, Uint8Array
 */
export interface ImageContentPart {
  type: 'image';
  /** Image data as URL string, base64 string, data URL, ArrayBuffer, or Uint8Array */
  image: string | URL | ArrayBuffer | Uint8Array;
  /** MIME type of the image (e.g., 'image/jpeg', 'image/png', 'image/gif', 'image/webp') */
  mimeType?: string;
  /** Detail level for vision models (OpenAI specific: 'auto', 'low', 'high') */
  detail?: 'auto' | 'low' | 'high';
}

/**
 * File content part for document attachments (PDFs, audio, etc.)
 */
export interface FileContentPart {
  type: 'file';
  /** File data as URL string, base64 string, ArrayBuffer, or Uint8Array */
  file: string | URL | ArrayBuffer | Uint8Array;
  /** MIME type of the file (required, e.g., 'application/pdf', 'audio/mp3') */
  mimeType: string;
  /** Optional filename for display purposes */
  filename?: string;
}

/**
 * Union type for all content parts in multimodal messages
 */
export type ContentPart = TextContentPart | ImageContentPart | FileContentPart;

/**
 * Message content can be a simple string, null, or an array of content parts
 * This maintains backward compatibility while enabling multimodal content
 */
export type MessageContent = string | null | ContentPart[];

// ============================================
// Reasoning Model Types
// ============================================

/**
 * Configuration options for reasoning models (o1/o3, Claude extended thinking, DeepSeek reasoner)
 */
export interface ReasoningConfig {
  /** Maximum tokens budget for reasoning/thinking (e.g., OpenAI's max_completion_tokens) */
  thinkingBudget?: number;
  /** Whether to include reasoning content in the response (default: true for reasoning models) */
  includeReasoning?: boolean;
  /** Reasoning effort level for OpenAI o-series models ('low', 'medium', 'high') */
  reasoningEffort?: 'low' | 'medium' | 'high';
}

/**
 * Reasoning/thinking content from model response
 */
export interface ReasoningContent {
  /** The reasoning/thinking content generated by the model */
  thinking: string;
  /** Signature for verification (Anthropic extended thinking) */
  signature?: string;
}

export interface ChatMessage {
  role: 'system' | 'user' | 'assistant' | 'function' | 'tool';
  /** Message content - can be string, null, or array of content parts for multimodal */
  content: MessageContent;
  name?: string;
  tool_calls?: Array<{
    id: string;
    type: 'function';
    function: {
      name: string;
      arguments: string;
    };
  }>;
  tool_call_id?: string;
  /** Reasoning/thinking content from reasoning models (o1/o3, Claude extended thinking, DeepSeek reasoner) */
  reasoning?: ReasoningContent;
}

export interface ChatCompletionOptions {
  messages: ChatMessage[];

  model?: string;
  temperature?: number;
  top_p?: number;
  max_tokens?: number;
  stream?: boolean;
  tools?: Array<{
    type: 'function';
    function: {
      name: string;
      description: string;
      parameters: Record<string, any>;
    };
  }>;
  stop?: string | string[];
  supportTools?: boolean;
  tool_choice?: 'auto' | 'none' | { type: 'function'; function: { name: string } };

  // Caching options
  /** Enable prompt caching (where supported by provider) */
  enableCaching?: boolean;
  /** Cache control for messages (Anthropic) */
  cacheControl?: CacheControl;
  /** System prompt cache control (Anthropic) */
  systemCacheControl?: CacheControl;
  /** Tool definitions cache control (Anthropic) */
  toolsCacheControl?: CacheControl;

  // Reasoning options
  /** Configuration for reasoning models (o1/o3, Claude extended thinking, DeepSeek reasoner) */
  reasoning?: ReasoningConfig;
}

export interface ChatCompletionResponse {
  id: string;
  object: string;
  created: number;
  model: string;
  choices: Array<{
    index: number;
    message: ChatMessage;
    delta?: {
      role?: string;
      content?: string;
      /** Reasoning content delta for streaming thinking models */
      reasoning?: string;
    };
    finish_reason: 'stop' | 'length' | 'tool_calls' | 'content_filter' | null;
  }>;
  usage: {
    prompt_tokens: number;
    completion_tokens: number;
    total_tokens: number;
    /** Tokens read from cache */
    cached_tokens?: number;
    /** Tokens written to cache (Anthropic) */
    cache_creation_tokens?: number;
    /** Cache hit tokens (Deepseek) */
    cache_hit_tokens?: number;
    /** Cache miss tokens (Deepseek) */
    cache_miss_tokens?: number;
    /** Reasoning/thinking tokens used by reasoning models */
    reasoning_tokens?: number;
    /** Cached reasoning tokens (OpenAI o-series) */
    reasoning_tokens_cached?: number;
    /** Raw provider-specific usage data */
    provider_usage?: Record<string, any>;
  };
}

export interface LLMProvider extends BaseProvider {
  provider?: SupportedProvider;
  createCompletion(options: ChatCompletionOptions): Promise<ChatCompletionResponse>;
  getModels(): Promise<any>;
  /** Create embeddings for text input */
  createEmbedding?(options: EmbeddingOptions): Promise<EmbeddingResponse>;
}

// ============================================
// Embedding Types
// ============================================

/**
 * Options for creating embeddings
 */
export interface EmbeddingOptions {
  /** Text input to embed - can be a single string or array of strings */
  input: string | string[];
  /** Model to use for embeddings (provider-specific) */
  model?: string;
  /** Encoding format for the embeddings (default: 'float') */
  encoding_format?: 'float' | 'base64';
  /** Number of dimensions for the output embeddings (if supported) */
  dimensions?: number;
  /** User identifier for tracking (OpenAI) */
  user?: string;
}

/**
 * Single embedding object
 */
export interface Embedding {
  /** The index of the embedding in the input array */
  index: number;
  /** The embedding vector */
  embedding: number[];
  /** Object type (always 'embedding') */
  object: 'embedding';
}

/**
 * Response from embedding API
 */
export interface EmbeddingResponse {
  /** Object type (always 'list') */
  object: 'list';
  /** Array of embedding objects */
  data: Embedding[];
  /** Model used for embeddings */
  model: string;
  /** Usage statistics */
  usage: {
    prompt_tokens: number;
    total_tokens: number;
  };
}

// ============================================
// Image Generation Types
// ============================================

/**
 * Options for image generation
 */
export interface ImageGenerationOptions {
  /** Text prompt describing the image to generate */
  prompt: string;
  /** Model to use (e.g., 'dall-e-3', 'dall-e-2') */
  model?: string;
  /** Number of images to generate (1-10, default: 1) */
  n?: number;
  /** Image size (e.g., '1024x1024', '1792x1024', '512x512') */
  size?: '256x256' | '512x512' | '1024x1024' | '1792x1024' | '1024x1792' | string;
  /** Quality level (standard or hd, DALL-E 3 only) */
  quality?: 'standard' | 'hd';
  /** Style preference (vivid or natural, DALL-E 3 only) */
  style?: 'vivid' | 'natural';
  /** Response format */
  response_format?: 'url' | 'b64_json';
  /** User identifier for tracking */
  user?: string;
}

/**
 * Individual generated image
 */
export interface GeneratedImage {
  /** URL of generated image (if response_format is 'url') */
  url?: string;
  /** Base64-encoded image data (if response_format is 'b64_json') */
  b64_json?: string;
  /** Revised prompt (DALL-E 3 returns the revised prompt used) */
  revised_prompt?: string;
}

/**
 * Response from image generation API
 */
export interface ImageGenerationResponse {
  /** Unix timestamp of creation */
  created: number;
  /** Array of generated images */
  data: GeneratedImage[];
}

// ============================================
// Reranking Types
// ============================================

/**
 * Options for reranking documents
 */
export interface RerankOptions {
  /** The search query to rank documents against */
  query: string;
  /** Documents to rerank - can be strings or objects with text field */
  documents: string[] | Array<{ text: string; [key: string]: any }>;
  /** Model to use for reranking */
  model?: string;
  /** Maximum number of results to return */
  top_n?: number;
  /** Whether to return documents in the response */
  return_documents?: boolean;
  /** Maximum number of tokens per document (truncates if exceeded) */
  max_chunks_per_doc?: number;
}

/**
 * Single reranked result
 */
export interface RerankResult {
  /** Original index in input documents array */
  index: number;
  /** Relevance score (higher is more relevant, typically 0-1) */
  relevance_score: number;
  /** The document text (if return_documents is true) */
  document?: { text: string };
}

/**
 * Response from reranking API
 */
export interface RerankResponse {
  /** Unique identifier for the request */
  id?: string;
  /** Array of reranked results, sorted by relevance */
  results: RerankResult[];
  /** Usage/billing information */
  meta?: {
    api_version?: { version: string };
    billed_units?: { search_units: number };
  };
}

// ============================================
// Transcription Types (Speech-to-Text)
// ============================================

/**
 * Options for audio transcription
 */
export interface TranscriptionOptions {
  /** Audio file to transcribe - can be File, Blob, Buffer, or URL */
  audio: File | Blob | ArrayBuffer | string;
  /** Model to use (e.g., 'whisper-1') */
  model?: string;
  /** Language of the audio in ISO-639-1 format (e.g., 'en', 'es') */
  language?: string;
  /** Prompt to guide the transcription style */
  prompt?: string;
  /** Response format: 'json', 'text', 'srt', 'vtt', 'verbose_json' */
  response_format?: 'json' | 'text' | 'srt' | 'vtt' | 'verbose_json';
  /** Temperature for sampling (0-1) */
  temperature?: number;
  /** Timestamp granularity for verbose_json: 'word' or 'segment' */
  timestamp_granularities?: ('word' | 'segment')[];
}

/**
 * Word-level timestamp
 */
export interface TranscriptionWord {
  word: string;
  start: number;
  end: number;
}

/**
 * Segment-level timestamp
 */
export interface TranscriptionSegment {
  id: number;
  seek: number;
  start: number;
  end: number;
  text: string;
  tokens: number[];
  temperature: number;
  avg_logprob: number;
  compression_ratio: number;
  no_speech_prob: number;
}

/**
 * Response from transcription API
 */
export interface TranscriptionResponse {
  /** The transcribed text */
  text: string;
  /** Task type (always 'transcribe') */
  task?: string;
  /** Detected or specified language */
  language?: string;
  /** Duration of the audio in seconds */
  duration?: number;
  /** Word-level timestamps (if requested) */
  words?: TranscriptionWord[];
  /** Segment-level timestamps (if requested) */
  segments?: TranscriptionSegment[];
}

// ============================================
// Speech Types (Text-to-Speech)
// ============================================

/**
 * Options for text-to-speech generation
 */
export interface SpeechOptions {
  /** Text to convert to speech */
  input: string;
  /** Model to use (e.g., 'tts-1', 'tts-1-hd') */
  model?: string;
  /** Voice to use (e.g., 'alloy', 'echo', 'fable', 'onyx', 'nova', 'shimmer') */
  voice?: 'alloy' | 'echo' | 'fable' | 'onyx' | 'nova' | 'shimmer' | string;
  /** Audio output format */
  response_format?: 'mp3' | 'opus' | 'aac' | 'flac' | 'wav' | 'pcm';
  /** Speed of speech (0.25 to 4.0, default 1.0) */
  speed?: number;
}

/**
 * Response from speech API
 */
export interface SpeechResponse {
  /** Audio data as ArrayBuffer */
  audio: ArrayBuffer;
  /** Content type of the audio (e.g., 'audio/mpeg') */
  contentType: string;
}

export type ToolSchema = {
  name: string;
  description: string;
  input_schema: {
    type: "object";
    properties: {
      [key: string]: {
        type: string;
        description: string;
      };
    };
    required: string[];
  };
};





export interface ToolCall {
  id: string;
  type: 'function';
  function: {
    name: string;
    arguments: string;
  };
}

export interface FunctionCall {
  name: string;
  arguments: string;
}



export interface Tool {
  type: 'function';
  function: {
    name: string;
    description?: string;
    parameters?: any;
  };
}



export interface Choice {
  index: number;
  message?: ChatMessage;
  delta?: ChatMessage;
  finish_reason?: 'stop' | 'length' | 'tool_calls' | 'content_filter' | 'function_call';
}

export interface Usage {
  prompt_tokens: number;
  completion_tokens: number;
  total_tokens: number;
}

// ============================================
// Caching Types
// ============================================

/**
 * Enhanced usage with caching information
 * Supports cache tracking across different providers
 */
export interface EnhancedUsage extends Usage {
  /** Tokens read from cache (OpenAI: cached_tokens, Anthropic: cache_read_input_tokens) */
  cached_tokens?: number;
  /** Tokens written to cache (Anthropic: cache_creation_input_tokens) */
  cache_creation_tokens?: number;
  /** Cache hit tokens (Deepseek: prompt_cache_hit_tokens) */
  cache_hit_tokens?: number;
  /** Cache miss tokens (Deepseek: prompt_cache_miss_tokens) */
  cache_miss_tokens?: number;
  /** Raw provider-specific usage data */
  provider_usage?: Record<string, any>;
}

/**
 * Cache control configuration for Anthropic
 */
export interface CacheControl {
  /** Type of cache control */
  type: 'ephemeral';
  /** Optional TTL - can be "1h" for 1 hour (Anthropic) */
  ttl?: string;
}

/**
 * Message content with cache control (for Anthropic)
 */
export interface CacheableContent {
  type: 'text';
  text: string;
  cache_control?: CacheControl;
}

/**
 * Extended message type with caching support
 */
export interface CacheableChatMessage extends ChatMessage {
  /** Cache control for this message (Anthropic) */
  cache_control?: CacheControl;
}

/**
 * Caching options for completion requests
 */
export interface CachingOptions {
  /** Enable prompt caching (where supported) */
  enableCaching?: boolean;
  /** Cache control configuration (Anthropic) */
  cacheControl?: CacheControl;
  /** System prompt cache control (Anthropic) */
  systemCacheControl?: CacheControl;
  /** Tool definitions cache control (Anthropic) */
  toolsCacheControl?: CacheControl;
}



// Anthropic Types
export interface AnthropicMessage {
  role: 'user' | 'assistant';
  content: string | AnthropicContent[];
}

export interface AnthropicContent {
  type: 'text' | 'tool_use' | 'tool_result';
  text?: string;
  id?: string;
  name?: string;
  input?: any;
  content?: string;
  tool_use_id?: string;
  is_error?: boolean;
  /** Cache control for prompt caching */
  cache_control?: CacheControl;
}

export interface AnthropicTool {
  name: string;
  description?: string;
  input_schema: any;
}

export interface AnthropicRequest {
  model: string;
  max_tokens: number;
  messages: AnthropicMessage[];
  system?: string;
  temperature?: number;
  top_p?: number;
  tools?: AnthropicTool[];
  tool_choice?: { type: 'auto' | 'any' | 'tool'; name?: string };
  stop_sequences?: string[];
  stream?: boolean;
}

export interface AnthropicResponse {
  id: string;
  type: 'message';
  role: 'assistant';
  content: AnthropicContent[];
  model: string;
  stop_reason: 'end_turn' | 'max_tokens' | 'stop_sequence' | 'tool_use';
  stop_sequence?: string;
  usage: {
    input_tokens: number;
    output_tokens: number;
  };
}

// Error handling types
export interface LLMError {
  message: string;
  type?: string;
  code?: string;
  provider?: string;
  model?: string;
  suggestion?: string;
}

export interface LLMErrorResponse {
  error: LLMError;
}

// ============================================
// Streaming Types
// ============================================

/**
 * Stream chunk format (OpenAI-compatible)
 * Represents a single chunk in a streaming response
 */
export interface StreamChunk {
  id: string;
  object: 'chat.completion.chunk';
  created: number;
  model: string;
  choices: Array<{
    index: number;
    delta: {
      role?: 'assistant';
      content?: string;
      /** Reasoning content delta for thinking models */
      reasoning?: string;
      tool_calls?: Array<{
        index: number;
        id?: string;
        type?: 'function';
        function?: {
          name?: string;
          arguments?: string;
        };
      }>;
    };
    finish_reason: 'stop' | 'length' | 'tool_calls' | 'content_filter' | null;
  }>;
  usage?: {
    prompt_tokens: number;
    completion_tokens: number;
    total_tokens: number;
    /** Reasoning tokens used */
    reasoning_tokens?: number;
  };
}

/**
 * Options for streaming completions
 * Extends ChatCompletionOptions with streaming-specific callbacks
 */
export interface StreamingOptions extends ChatCompletionOptions {
  /** Called for each chunk received */
  onChunk?: (chunk: StreamChunk) => void;
  /** Called when streaming completes with full aggregated response */
  onComplete?: (response: ChatCompletionResponse) => void;
  /** Called if an error occurs during streaming */
  onError?: (error: Error) => void;
  /** AbortSignal for cancellation support */
  signal?: AbortSignal;
}

/**
 * Provider capability flags
 * Used to check what features a provider supports
 */
export interface ProviderCapabilities {
  supportsStreaming: boolean;
  supportsTools: boolean;
  supportsVision: boolean;
  supportsEmbeddings: boolean;
  /** Whether the provider supports prompt caching */
  supportsCaching: boolean;
  /** Type of caching: 'automatic' (OpenAI, Deepseek) or 'explicit' (Anthropic) */
  cachingType?: 'automatic' | 'explicit' | 'none';
  /** Whether provider supports image generation */
  supportsImageGeneration?: boolean;
  /** Whether provider supports document reranking */
  supportsReranking?: boolean;
  /** Whether provider supports audio transcription (speech-to-text) */
  supportsTranscription?: boolean;
  /** Whether provider supports speech synthesis (text-to-speech) */
  supportsSpeech?: boolean;
  /** Whether provider supports reasoning models (o1/o3, extended thinking, deepseek-reasoner) */
  supportsReasoning?: boolean;
  /** Whether provider supports multimodal content (images, files in messages) */
  supportsMultimodal?: boolean;
}

/**
 * Extended LLMProvider interface with streaming support
 */
export interface LLMProviderWithStreaming extends LLMProvider {
  /** Stream completion with callback-based chunks */
  createCompletionStream?(options: StreamingOptions): Promise<ChatCompletionResponse>;
  /** AsyncGenerator-based streaming */
  streamCompletion?(options: ChatCompletionOptions): AsyncGenerator<StreamChunk, void, unknown>;
  /** Get provider capabilities */
  getCapabilities?(): ProviderCapabilities;
  /** Generate images from text prompt */
  createImage?(options: ImageGenerationOptions): Promise<ImageGenerationResponse>;
  /** Rerank documents by relevance to query */
  rerank?(options: RerankOptions): Promise<RerankResponse>;
  /** Transcribe audio to text */
  createTranscription?(options: TranscriptionOptions): Promise<TranscriptionResponse>;
  /** Convert text to speech */
  createSpeech?(options: SpeechOptions): Promise<SpeechResponse>;
}
